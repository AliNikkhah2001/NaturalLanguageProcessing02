{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8552736,"sourceType":"datasetVersion","datasetId":5111013},{"sourceId":8552920,"sourceType":"datasetVersion","datasetId":5111154},{"sourceId":8553306,"sourceType":"datasetVersion","datasetId":5111403},{"sourceId":8553867,"sourceType":"datasetVersion","datasetId":5111759}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Titillium+Web:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700&display=swap\" rel=\"stylesheet\">\n<div class=\"box\" style=\"padding: 10px; margin: 100px 90px; background-color: white; color: white; border-radius: 5px; font-size: 15px; box-shadow: rgba(0, 0, 0, 0.25) 0px 54px 55px, rgba(0, 0, 0, 0.12) 0px -12px 30px, rgba(0, 0, 0, 0.12) 0px 4px 6px, rgba(0, 0, 0, 0.17) 0px 12px 13px, rgba(0, 0, 0, 0.09) 0px -3px 5px;\">\n  <table style=\"padding: 10px; margin: auto auto;   border-radius: 20px; font-size: 15px;\">\n      <tr>\n          <th colspan='2'><h1 style=\"text-align: center\">\nNatural Language Processing</br>\n</h1>\n<h2 style=\"text-align: center\">\nCourse Assignment Two </br>\n</h2>\n</th>\n<tr>\n    <tr>\n      <th colspan=\"2\">Personal Info</th>\n    </tr>\n    <tr>\n      <td>Ali</td>\n    <td><b>Nikkhah</b></td>\n    </tr>\n    <tr>\n      <td>Sarina</td>\n    <td><b>Zahedi</b></td>\n    </tr>\n    <tr>\n      <td>Ramtin</td>\n    <td><b>Khoshnevis</b></td>\n    </tr>\n    <tr>\n      <td>Github:</td>\n      <td><a href=\"https://github.com/AliNikkhah2001/NaturalLanguageProcessing02\" target=\"_blank\">https://github.com/AliNikkhah2001/NaturalLanguageProcessing02</a></td>\n    </tr>\n  </table>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Initializations","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install transformers datasets torch\n!pip install pandas tqdm\n!pip install python-Levenshtein\n!pip install jellyfish\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T03:58:54.011304Z","iopub.execute_input":"2024-05-30T03:58:54.012044Z","iopub.status.idle":"2024-05-30T03:59:46.360185Z","shell.execute_reply.started":"2024-05-30T03:58:54.012014Z","shell.execute_reply":"2024-05-30T03:59:46.358935Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries from Hugging Face and PyTorch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset, Dataset\nfrom tqdm.notebook import tqdm \nimport random\nimport torch\nimport pandas as pd\nimport os\nimport re\nimport Levenshtein\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:23:36.995946Z","iopub.execute_input":"2024-05-30T04:23:36.996486Z","iopub.status.idle":"2024-05-30T04:23:37.003317Z","shell.execute_reply.started":"2024-05-30T04:23:36.996444Z","shell.execute_reply":"2024-05-30T04:23:37.002275Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:23:37.178147Z","iopub.execute_input":"2024-05-30T04:23:37.178857Z","iopub.status.idle":"2024-05-30T04:23:38.288497Z","shell.execute_reply.started":"2024-05-30T04:23:37.178822Z","shell.execute_reply":"2024-05-30T04:23:38.287276Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Thu May 30 04:23:38 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   44C    P0              35W / 250W |  10558MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load of the Datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:34:37.242119Z","iopub.execute_input":"2024-05-29T18:34:37.242571Z","iopub.status.idle":"2024-05-29T18:34:37.248039Z","shell.execute_reply.started":"2024-05-29T18:34:37.242533Z","shell.execute_reply":"2024-05-29T18:34:37.246846Z"}}},{"cell_type":"code","source":"datasetPath=\"/kaggle/input/nlpsphw2\"\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:23:38.290918Z","iopub.execute_input":"2024-05-30T04:23:38.291276Z","iopub.status.idle":"2024-05-30T04:23:38.296050Z","shell.execute_reply.started":"2024-05-30T04:23:38.291237Z","shell.execute_reply":"2024-05-30T04:23:38.295146Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Error in words dataset","metadata":{}},{"cell_type":"code","source":"\n# Define the paths to the datasets\nfaspell_main_path = datasetPath + '/faspell_main.txt'\nfaspell_ocr_path = datasetPath + '/faspell_ocr.txt'\n\n# Read and clean the faspell_main dataset\nfaspell_main_df = pd.read_csv(faspell_main_path, sep='\\t', encoding='utf-8', header=None, names=[\"misspelled\", \"corrected\", \"error-category\"])\nfaspell_main_cleaned_df = faspell_main_df.rename(columns={\"corrected\": \"wrong\", \"misspelled\": \"correct\"})\nfaspell_main_cleaned_df = faspell_main_cleaned_df[['correct', 'wrong']]\n\n# Read and clean the faspell_ocr dataset\nfaspell_ocr_df = pd.read_csv(faspell_ocr_path, sep='\\t', encoding='utf-8', header=None, names=[\"misspelled\", \"corrected\"])\nfaspell_ocr_cleaned_df = faspell_ocr_df.rename(columns={\"corrected\": \"wrong\", \"misspelled\": \"correct\"})\nfaspell_ocr_cleaned_df = faspell_ocr_cleaned_df[['correct', 'wrong']]\n\n# Concatenate the cleaned dataframes\nwords_df = pd.concat([faspell_main_cleaned_df, faspell_ocr_cleaned_df], ignore_index=True)\n\n# Print samples from the final DataFrame\nprint(words_df.sample(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:23:38.297456Z","iopub.execute_input":"2024-05-30T04:23:38.297814Z","iopub.status.idle":"2024-05-30T04:23:38.335620Z","shell.execute_reply.started":"2024-05-30T04:23:38.297779Z","shell.execute_reply":"2024-05-30T04:23:38.334744Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"      correct     wrong\n4055  نپیچیند    نپیچید\n5100      حوب       خوب\n3692  ميآوريم  مي آوريم\n1499    خارحي     خارجي\n3296     محمع      مجمع\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Synthetic dataset generation of typo","metadata":{}},{"cell_type":"code","source":"# Load the list of Persian words\ndistinct_words_path = '/kaggle/input/distinctwords/distinct_words.txt'\nwith open(distinct_words_path, 'r', encoding='utf-8') as file:\n    words = file.read().splitlines()\n\n# Functions to create typographical errors\ndef missing_char(word):\n    pos = random.randint(0, len(word) - 1)\n    return word[:pos] + word[pos + 1:]\n\ndef extra_char(word):\n    pos = random.randint(0, len(word))\n    char = random.choice('ابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی')\n    return word[:pos] + char + word[pos:]\n\ndef swapped_chars(word):\n    if len(word) < 5:\n        return word\n    pos1, pos2 = random.sample(range(len(word)), 2)\n    word_list = list(word)\n    word_list[pos1], word_list[pos2] = word_list[pos2], word_list[pos1]\n    return ''.join(word_list)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:23:38.337821Z","iopub.execute_input":"2024-05-30T04:23:38.338164Z","iopub.status.idle":"2024-05-30T04:23:38.467010Z","shell.execute_reply.started":"2024-05-30T04:23:38.338132Z","shell.execute_reply":"2024-05-30T04:23:38.465935Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"additional_data = []\nfor word in tqdm(words, desc=\"Generating synthetic data\"):\n    if(len(word)>5):\n        for _ in range(20):\n            wordtype = random.choice([missing_char, extra_char, swapped_chars])\n            gen_word = wordtype(word)\n            additional_data.append({'correct': gen_word, 'wrong': word})\n\n# Create a DataFrame from the additional data\nadditional_df = pd.DataFrame(additional_data)\n\n# Append the additional data to the original DataFrame\nwords_df = pd.concat([words_df, additional_df], ignore_index=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:23:38.468284Z","iopub.execute_input":"2024-05-30T04:23:38.468648Z","iopub.status.idle":"2024-05-30T04:24:25.045072Z","shell.execute_reply.started":"2024-05-30T04:23:38.468617Z","shell.execute_reply":"2024-05-30T04:24:25.044232Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating synthetic data:   0%|          | 0/453158 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63191f28e285491eb7083a3293a21483"}},"metadata":{}}]},{"cell_type":"markdown","source":"## ADD HE-KASRE TO DF","metadata":{}},{"cell_type":"code","source":"# Load the annotated dataset\nannotated_path = '/kaggle/input/nlpsphw2/annotated.csv'\nannotated_df = pd.read_csv(annotated_path, encoding='utf-8')\n\n# Extract the name of the single column\ncolumn_name = annotated_df.columns[0]\n\n# Function to process each row\ndef process_text(row):\n    text = row[column_name]\n    words = text.split()  # Split the text into words\n    correct_words = []\n    wrong_words = []\n    \n    for word in words:\n        if word.endswith('ه+'):\n            correct_word = word[:-2]  # Remove 'ه+'\n            wrong_word = word.replace('+', '')  # Remove only the '+'\n            correct_words.append(correct_word)\n            wrong_words.append(wrong_word)\n    \n    correct_text = ' '.join(correct_words)\n    wrong_text = ' '.join(wrong_words)\n    \n    return pd.Series([correct_text, wrong_text], index=['correct', 'wrong'])\n\n# Apply the process_text function to each row in the DataFrame\ntmp_df = annotated_df.apply(process_text, axis=1)\nwords_df = pd.concat([words_df, tmp_df], ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:25.047244Z","iopub.execute_input":"2024-05-30T04:24:25.047578Z","iopub.status.idle":"2024-05-30T04:24:25.491141Z","shell.execute_reply.started":"2024-05-30T04:24:25.047551Z","shell.execute_reply":"2024-05-30T04:24:25.490337Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Print samples from the final DataFrame\nprint(words_df.sample(20))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:25.492196Z","iopub.execute_input":"2024-05-30T04:24:25.492460Z","iopub.status.idle":"2024-05-30T04:24:25.745508Z","shell.execute_reply.started":"2024-05-30T04:24:25.492437Z","shell.execute_reply":"2024-05-30T04:24:25.744528Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"               correct         wrong\n6222240        تونشکین        تونکین\n6054334  نمازجمعذه‌است  نمازجمعه‌است\n1418416        رنجیابی      رنج‌یابی\n4104848     ناامیدششدن     ناامیدشدن\n499408        خان‌کنید      خان‌کندی\n1964587         مافورد       مامفورد\n2197184          کج‌قل        کج‌قلم\n1798982      روضه‌خانى    روضه‌خوانى\n4211377      هیپوتیزور    هیپنوتیزور\n3107284    کامبوجچی‌ها    کامبوجی‌ها\n5119134         فریازن       فریازان\n5426767        الناشدد        الناشد\n5547876   پایفبندی‌های   پایبندی‌های\n1969865        کالجدار        کالدار\n3802962       هاده‌اید     نهاده‌اید\n2171856       کردازینت       کردزینت\n4059989          بدعلی        بدعملی\n4431442     موزی‌ویدئو   موزیک‌ویدئو\n5019579       قایلیتاب      قابلیتای\n722993      رشته‌گرنفی    رشته‌فرنگی\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_size = words_df.size\nprint(f\"Size of the dataset: {dataset_size}\")\n\n# Print number of columns\nnum_columns = words_df.shape[1]\nprint(f\"Number of columns: {num_columns}\")\n\n# Print number of rows\nnum_rows = words_df.shape[0]\nprint(f\"Number of rows: {num_rows}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:25.746787Z","iopub.execute_input":"2024-05-30T04:24:25.747169Z","iopub.status.idle":"2024-05-30T04:24:25.753501Z","shell.execute_reply.started":"2024-05-30T04:24:25.747134Z","shell.execute_reply":"2024-05-30T04:24:25.752577Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Size of the dataset: 13296918\nNumber of columns: 2\nNumber of rows: 6648459\n","output_type":"stream"}]},{"cell_type":"code","source":"# Write the DataFrame to a tab-separated file\nwords_df_path = '/kaggle/working/created_words_dataset.txt'\nwords_df.to_csv(words_df_path, sep='\\t', index=False)\n\nprint(f\"DataFrame written to {words_df_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:25.754993Z","iopub.execute_input":"2024-05-30T04:24:25.755434Z","iopub.status.idle":"2024-05-30T04:24:36.491089Z","shell.execute_reply.started":"2024-05-30T04:24:25.755400Z","shell.execute_reply":"2024-05-30T04:24:36.490155Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"DataFrame written to /kaggle/working/created_words_dataset.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sentences Dataset","metadata":{}},{"cell_type":"code","source":"\nreal_word_path = '/kaggle/input/nlpsphw2/DataSet/DataSet/real-word'\nreal_word_files = ['gozar', 'plural', 'be', 'tanvin', 'hich', 'common']\nreal_word_files","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:36.495195Z","iopub.execute_input":"2024-05-30T04:24:36.495690Z","iopub.status.idle":"2024-05-30T04:24:36.504110Z","shell.execute_reply.started":"2024-05-30T04:24:36.495645Z","shell.execute_reply":"2024-05-30T04:24:36.503032Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"['gozar', 'plural', 'be', 'tanvin', 'hich', 'common']"},"metadata":{}}]},{"cell_type":"code","source":"\ndata = []\n\nfor file_name in real_word_files:\n    correct_file = os.path.join(real_word_path, f'{file_name}/correct_{file_name}.txt')\n    wrong_file = os.path.join(real_word_path, f'{file_name}/wrong_{file_name}.txt')\n    \n    with open(correct_file, 'r', encoding='utf-8') as f:\n        correct_lines = f.readlines()\n        \n    with open(wrong_file, 'r', encoding='utf-8') as f:\n        wrong_lines = f.readlines()\n    \n    for correct, wrong in zip(correct_lines, wrong_lines):\n        data.append({'correct': correct.strip(), 'wrong': wrong.strip()})\n\nsentences_df = pd.DataFrame(data)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:36.505832Z","iopub.execute_input":"2024-05-30T04:24:36.506174Z","iopub.status.idle":"2024-05-30T04:24:36.567026Z","shell.execute_reply.started":"2024-05-30T04:24:36.506150Z","shell.execute_reply":"2024-05-30T04:24:36.566345Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndehkhoda_corpus = pd.read_csv('/kaggle/input/dehkhoda/dehkhoda_corpus.csv')\n\n# Assuming the dataset has columns 'wrong' and 'correct'\nwrong = dehkhoda_corpus['wrong']\ncorrect = dehkhoda_corpus['correct']\n\n# Convert to DataFrame if needed\nwrong_df = wrong.to_frame(name='wrong')\ncorrect_df = correct.to_frame(name='correct')\n\n# Combine the wrong and correct DataFrames into one\ncombined_df = pd.concat([wrong_df, correct_df], axis=1)\n\n# Append to sentences_df\nsentences_df = pd.concat([sentences_df, combined_df], ignore_index=True)\n# Display the updated sentences_df\nprint(sentences_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:36.568159Z","iopub.execute_input":"2024-05-30T04:24:36.568536Z","iopub.status.idle":"2024-05-30T04:24:36.600878Z","shell.execute_reply.started":"2024-05-30T04:24:36.568491Z","shell.execute_reply":"2024-05-30T04:24:36.599976Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"                                             correct  \\\n0  اما گروه دوم افزایش تدریجی قابل پیش‌بینی و مشر...   \n1  دانشگاه‌ها حوزه اثرگذاری چه قبل از انقلاب و چه...   \n2         به‌اندازه ظرفیت خودش بر بازار اثرگذار باشد   \n3  که تشکیل دادسرای امور بین‌الملل برای رسیدگی به...   \n4  در صورت نیاز نسبت به اصلاح نسبت به اثرگذاری مت...   \n\n                                               wrong  \n0  اما گروه دوم افزایش تدریجی قابل پیش‌بینی و مشر...  \n1  دانشگاه‌ها حوزه اثر‌گزاری چه قبل از انقلاب و چ...  \n2        به‌اندازه ظرفیت خودش بر بازار اثر‌گزار باشد  \n3  که تشکیل دادسرای امور بین‌الملل برای رسیدگی به...  \n4  در صورت نیاز نسبت به اصلاح نسبت به اثر‌گزاری م...  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert to DataFrame\nsynthetic_df = pd.DataFrame(data)\nprint(synthetic_df.sample(5))\ndataset_size = synthetic_df.size\nprint(f\"Size of the dataset: {dataset_size}\")\n\n# Print number of columns\nnum_columns = synthetic_df.shape[1]\nprint(f\"Number of columns: {num_columns}\")\n\n# Print number of rows\nnum_rows = synthetic_df.shape[0]\nprint(f\"Number of rows: {num_rows}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:36.601941Z","iopub.execute_input":"2024-05-30T04:24:36.602235Z","iopub.status.idle":"2024-05-30T04:24:36.618506Z","shell.execute_reply.started":"2024-05-30T04:24:36.602189Z","shell.execute_reply":"2024-05-30T04:24:36.617506Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"                                                correct  \\\n1712      قانونگذار به این دلیل اختیار را به دولت نسپرد   \n2467  همین شکل و شمایل و آداب و هنر و رسوءم و پوشاک ...   \n39                              ما باج‌گزار شما می‌شویم   \n4976  و درباره موضعات مهمی از جمله چگونگی بهبود و ار...   \n6209    اگر سهامداران شرکت‌ها را متعلق به خودشان بدانند   \n\n                                                  wrong  \n1712       قانوگذار به این دلیل اختیار را به دولت نسپرد  \n2467  همین شکل و شمایل و آداب‌ها و هنر و رسوم و پوشا...  \n39                              ما باج‌گذار شما می‌شویم  \n4976  و درباره موضعات مهمی از جمله چگونگی بهبود و ار...  \n6209      اگر سهامداران شرکت‌ها را متعلق بخودشان بدانند  \nSize of the dataset: 15932\nNumber of columns: 2\nNumber of rows: 7966\n","output_type":"stream"}]},{"cell_type":"code","source":"sentences_df = pd.concat([sentences_df, synthetic_df], ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:36.619578Z","iopub.execute_input":"2024-05-30T04:24:36.619893Z","iopub.status.idle":"2024-05-30T04:24:36.627655Z","shell.execute_reply.started":"2024-05-30T04:24:36.619867Z","shell.execute_reply":"2024-05-30T04:24:36.626717Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"dataset_size = sentences_df.size\nprint(f\"Size of the dataset: {dataset_size}\")\n\n# Print number of columns\nnum_columns = sentences_df.shape[1]\nprint(f\"Number of columns: {num_columns}\")\n\n# Print number of rows\nnum_rows = sentences_df.shape[0]\nprint(f\"Number of rows: {num_rows}\")\n\n\nwords_df_path = 'created_sentences_dataset.txt'\nwords_df.to_csv(words_df_path, sep='\\t', index=False)\n\nprint(f\"DataFrame written to {words_df_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:36.628904Z","iopub.execute_input":"2024-05-30T04:24:36.629292Z","iopub.status.idle":"2024-05-30T04:24:47.415631Z","shell.execute_reply.started":"2024-05-30T04:24:36.629258Z","shell.execute_reply":"2024-05-30T04:24:47.414263Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Size of the dataset: 33280\nNumber of columns: 2\nNumber of rows: 16640\nDataFrame written to created_sentences_dataset.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset for Hekasre","metadata":{}},{"cell_type":"code","source":"annotated_path = datasetPath+'/annotated.csv'\nannotated_df = pd.read_csv(annotated_path, encoding='utf-8')\nannotated_df = pd.DataFrame(annotated_df)\n# Extract the name of the single column\ncolumn_name = annotated_df.columns[0]\n\n# Function to process each row\ndef process_text(row):\n    text = row[column_name]\n    correct_text = text.replace('ه+', '')\n    wrong_text = text.replace('+', '')\n    return pd.Series([correct_text, wrong_text], index=['correct', 'wrong'])\n\n# Apply the function to each row and create a new DataFrame\nresult_df = annotated_df.apply(process_text, axis=1)\nprint(result_df.sample(5))\ndataset_size = result_df.size\nprint(f\"Size of the dataset: {dataset_size}\")\n\n# Print number of columns\nnum_columns = result_df.shape[1]\nprint(f\"Number of columns: {num_columns}\")\n\n# Print number of rows\nnum_rows = result_df.shape[0]\nprint(f\"Number of rows: {num_rows}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:47.417030Z","iopub.execute_input":"2024-05-30T04:24:47.417780Z","iopub.status.idle":"2024-05-30T04:24:47.632758Z","shell.execute_reply.started":"2024-05-30T04:24:47.417750Z","shell.execute_reply":"2024-05-30T04:24:47.631854Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"                                               correct  \\\n515  بچه هفت سال دارد پدرش قصاب است با مادرش اختلاف...   \n723  حسین فریدون برادر رییس جمهور به اتهام دریافت ر...   \n124  اتهامات فریدون تاسیس صرافی دست داشتن در انتصاب...   \n549  بیژن قاسم زاده بازپرس سابق شعبه دوم دادسرای فر...   \n238  وقتش- که رهبر اغتشاشات فرانسه رو بیاریم قم و د...   \n\n                                                 wrong  \n515  بچه هفت سال دارد پدرش قصاب است با مادرش اختلاف...  \n723  حسین فریدون برادره رییس جمهور به اتهامه دریافت...  \n124  اتهاماته فریدون تاسیسه صرافی دست داشتن در انتص...  \n549  بیژنه قاسم زاده بازپرسه سابقه شعبه دوم دادسرای...  \n238  وقتش- که رهبر اغتشاشاته فرانسه رو بیاریم قم و ...  \nSize of the dataset: 1998\nNumber of columns: 2\nNumber of rows: 999\n","output_type":"stream"}]},{"cell_type":"code","source":"sentences_df = pd.concat([sentences_df, result_df], ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:47.634133Z","iopub.execute_input":"2024-05-30T04:24:47.634569Z","iopub.status.idle":"2024-05-30T04:24:47.640743Z","shell.execute_reply.started":"2024-05-30T04:24:47.634532Z","shell.execute_reply":"2024-05-30T04:24:47.639817Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"dataset_size = sentences_df.size\nprint(f\"Size of the dataset: {dataset_size}\")\n\n# Print number of columns\nnum_columns = sentences_df.shape[1]\nprint(f\"Number of columns: {num_columns}\")\n\n# Print number of rows\nnum_rows = sentences_df.shape[0]\nprint(f\"Number of rows: {num_rows}\")\n\n\nwords_df_path = 'created_sentences_dataset.txt'\nwords_df.to_csv(words_df_path, sep='\\t', index=False)\n\nprint(f\"DataFrame written to {words_df_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:47.641994Z","iopub.execute_input":"2024-05-30T04:24:47.642426Z","iopub.status.idle":"2024-05-30T04:24:58.507687Z","shell.execute_reply.started":"2024-05-30T04:24:47.642395Z","shell.execute_reply":"2024-05-30T04:24:58.506738Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Size of the dataset: 35278\nNumber of columns: 2\nNumber of rows: 17639\nDataFrame written to created_sentences_dataset.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Distance Model","metadata":{}},{"cell_type":"code","source":"import jellyfish","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:58.509058Z","iopub.execute_input":"2024-05-30T04:24:58.509660Z","iopub.status.idle":"2024-05-30T04:24:58.513473Z","shell.execute_reply.started":"2024-05-30T04:24:58.509631Z","shell.execute_reply":"2024-05-30T04:24:58.512607Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"import Levenshtein\nimport jellyfish\nimport pandas as pd\n\nclass DistanceModel:\n    def __init__(self, words_path, words_df_path=None):\n        \"\"\"\n        Initializes the DistanceModel with a list of Persian words and a correction DataFrame.\n        \n        Args:\n        words_path (str): Path to the file containing Persian words.\n        words_df_path (str): Path to the file containing the correction DataFrame.\n        \"\"\"\n        self.words = self.load_words(words_path)\n        self.words_set = set(self.words)  # Use a set for O(1) average-time complexity lookups\n        self.soundex_dict = self.build_soundex_dict(self.words)\n\n    def load_words(self, path):\n        \"\"\"\n        Loads words from a specified file.\n        \n        Args:\n        path (str): Path to the file containing Persian words.\n        \n        Returns:\n        list: List of words.\n        \"\"\"\n        with open(path, 'r', encoding='utf-8') as file:\n            words = file.read().splitlines()\n        return words\n\n    def load_words_df(self, path):\n        \"\"\"\n        Loads the correction DataFrame from a specified file.\n        \n        Args:\n        path (str): Path to the file containing the correction DataFrame.\n        \n        Returns:\n        DataFrame: DataFrame containing 'wrong' and 'correct' columns.\n        \"\"\"\n        return pd.read_csv(path, sep='\\t')  # Assuming the file is tab-separated\n\n    def build_soundex_dict(self, words):\n        \"\"\"\n        Builds a dictionary mapping Soundex codes to lists of words.\n        \n        Args:\n        words (list): List of words to build the Soundex dictionary.\n        \n        Returns:\n        dict: Dictionary mapping Soundex codes to lists of words.\n        \"\"\"\n        soundex_dict = {}\n        for word in words:\n            soundex_code = jellyfish.soundex(word)\n            if soundex_code not in soundex_dict:\n                soundex_dict[soundex_code] = []\n            soundex_dict[soundex_code].append(word)\n        return soundex_dict\n\n    def calculate_distance(self, word1, word2):\n        \"\"\"\n        Calculates the Levenshtein distance between two words.\n        \n        Args:\n        word1 (str): The first word.\n        word2 (str): The second word.\n        \n        Returns:\n        int: The Levenshtein distance between the two words.\n        \"\"\"\n        return Levenshtein.distance(word1, word2)\n\n    def find_closest_word(self, word):\n        \"\"\"\n        Finds the closest word in the loaded list of Persian words to the input word.\n        \n        Args:\n        word (str): The input word to find the closest match for.\n        \n        Returns:\n        str: The closest word from the list of Persian words.\n        \"\"\"\n        soundex_code = jellyfish.soundex(word)\n        candidate_words = self.soundex_dict.get(soundex_code, self.words)\n        \n        min_distance = float('inf')\n        closest_word = None\n        \n        for persian_word in candidate_words:\n            distance = self.calculate_distance(word, persian_word)\n            if distance < min_distance:\n                min_distance = distance\n                closest_word = persian_word\n        \n        return closest_word\n\n    def correct_words(self, words):\n        \"\"\"\n        Corrects words in the input list using the words_df DataFrame.\n        \n        Args:\n        words (list): List of words to be corrected.\n        \n        Returns:\n        list: List of corrected words.\n        \"\"\"\n        wrong_to_correct = dict(zip(self.words_df['wrong'], self.words_df['correct']))\n        corrected_words = [wrong_to_correct.get(word, word) for word in words]\n        return corrected_words\n\n    def inference(self, sentence):\n        \"\"\"\n        Processes the input sentence and returns a cleaned version with words replaced \n        by their closest matches from the list of Persian words.\n        \n        Args:\n        sentence (str): The input sentence to be cleaned.\n        \n        Returns:\n        str: The cleaned sentence with words replaced by their closest matches.\n        \"\"\"\n        # Split the sentence into words\n        words = sentence.split()\n        \n        # Correct the words using the words_df DataFrame\n        #words = self.correct_words(words)\n        \n        # Replace each word with its closest match from the list of Persian words\n        cleaned_sentence = ' '.join(word if word in self.words_set else self.find_closest_word(word) for word in words)\n        \n        return cleaned_sentence\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:58.531888Z","iopub.execute_input":"2024-05-30T04:24:58.532490Z","iopub.status.idle":"2024-05-30T04:24:59.624626Z","shell.execute_reply.started":"2024-05-30T04:24:58.532459Z","shell.execute_reply":"2024-05-30T04:24:59.623408Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"created_sentences_dataset.txt  created_words_dataset.txt  results\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example usage:\nwords_path = '/kaggle/input/persianwords/big.txt'\nwords_df_path = '/kaggle/working/created_words_dataset.txt'  \n\nmodel = DistanceModel(words_path, words_df_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:24:59.629906Z","iopub.execute_input":"2024-05-30T04:24:59.630244Z","iopub.status.idle":"2024-05-30T04:25:01.080723Z","shell.execute_reply.started":"2024-05-30T04:24:59.630201Z","shell.execute_reply":"2024-05-30T04:25:01.079588Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Sample sentence for inference\nsample_sentence = \"منه بهه خیلیی میرویدی\"\ncleaned_sentence = model.inference(sample_sentence)\nprint(cleaned_sentence)\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:25:01.082082Z","iopub.execute_input":"2024-05-30T04:25:01.082500Z","iopub.status.idle":"2024-05-30T04:25:01.267333Z","shell.execute_reply.started":"2024-05-30T04:25:01.082450Z","shell.execute_reply":"2024-05-30T04:25:01.266325Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"منه بهره خیلی میروید\n\n","output_type":"stream"}]},{"cell_type":"code","source":"input_sentence = input(\"Enter a sentence: \")\ncleaned_sentence = model.inference(sample_sentence)\nprint(\"Corrected Sentence:\", input_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:25:01.268391Z","iopub.execute_input":"2024-05-30T04:25:01.268647Z","iopub.status.idle":"2024-05-30T04:27:15.377769Z","shell.execute_reply.started":"2024-05-30T04:25:01.268624Z","shell.execute_reply":"2024-05-30T04:27:15.376703Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a sentence:  \n"},{"name":"stdout","text":"Corrected Sentence: \n","output_type":"stream"}]},{"cell_type":"code","source":"sample_sentences = [\n    \"هن به مدسه رفتم\",\n    \"امروز هوا خیلیی خوب است\",\n    \"من کتاب می‌خوامنم\",\n    \"دیروز با دوستمم به پارک رفتم\",\n    \"من عاقش یادگری هسنم\"\n]\n\nfor sentence in sample_sentences:\n    corrected_sentence = model.inference(sentence)\n    print(f\"Original: {sentence}\")\n    print(f\"Corrected: {corrected_sentence}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:15.379153Z","iopub.execute_input":"2024-05-30T04:27:15.379556Z","iopub.status.idle":"2024-05-30T04:27:15.576827Z","shell.execute_reply.started":"2024-05-30T04:27:15.379503Z","shell.execute_reply":"2024-05-30T04:27:15.575838Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Original: هن به مدسه رفتم\nCorrected: هن به مدسه رفتم\n\nOriginal: امروز هوا خیلیی خوب است\nCorrected: امروز هوا خیلی خوب است\n\nOriginal: من کتاب می‌خوامنم\nCorrected: من کتاب می‌خوانم\n\nOriginal: دیروز با دوستمم به پارک رفتم\nCorrected: دیروز با دوستم به پارک رفتم\n\nOriginal: من عاقش یادگری هسنم\nCorrected: من عاقل یادگیری هستم\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# T5 Model Training\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator, DataLoaderConfiguration\nimport os\n\n\n# Disable wandb logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Model and tokenizer\nmodel_name = \"Ahmad/parsT5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:15.577974Z","iopub.execute_input":"2024-05-30T04:27:15.578285Z","iopub.status.idle":"2024-05-30T04:27:17.518629Z","shell.execute_reply.started":"2024-05-30T04:27:15.578260Z","shell.execute_reply":"2024-05-30T04:27:17.517587Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Adding special tokens to the tokenizer\nspecial_tokens_dict = {'additional_special_tokens': ['<extra_token_1>', '<extra_token_2>']}\ntokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:17.520018Z","iopub.execute_input":"2024-05-30T04:27:17.520359Z","iopub.status.idle":"2024-05-30T04:27:18.048519Z","shell.execute_reply.started":"2024-05-30T04:27:17.520331Z","shell.execute_reply":"2024-05-30T04:27:18.047454Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"Embedding(32105, 768)"},"metadata":{}}]},{"cell_type":"code","source":"words_df = pd.DataFrame(words_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:18.049831Z","iopub.execute_input":"2024-05-30T04:27:18.050228Z","iopub.status.idle":"2024-05-30T04:27:18.055002Z","shell.execute_reply.started":"2024-05-30T04:27:18.050173Z","shell.execute_reply":"2024-05-30T04:27:18.054066Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass WordCorrectionDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        input_text = self.dataframe.iloc[idx][\"wrong\"]\n        target_text = self.dataframe.iloc[idx][\"correct\"]\n        input_encoding = self.tokenizer(\n            input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        target_encoding = self.tokenizer(\n            target_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        labels = target_encoding[\"input_ids\"]\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": input_encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": input_encoding[\"attention_mask\"].flatten(),\n            \"labels\": labels.flatten(),\n        }\n\n    def print_samples(self, num_samples=5):\n        \"\"\"Prints a specified number of samples from the dataset.\"\"\"\n        print(f\"{'Index':<10}{'Wrong Word':<20}{'Correct Word'}\")\n        print(\"=\"*50)\n        for idx in range(min(num_samples, len(self.dataframe))):\n            wrong = self.dataframe.iloc[idx][\"wrong\"]\n            correct = self.dataframe.iloc[idx][\"correct\"]\n            print(f\"{idx:<10}{wrong:<20}{correct}\")\n\nclass SentenceCorrectionDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        input_text = self.dataframe.iloc[idx][\"wrong\"]\n        target_text = self.dataframe.iloc[idx][\"correct\"]\n        input_encoding = self.tokenizer(\n            input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        target_encoding = self.tokenizer(\n            target_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        labels = target_encoding[\"input_ids\"]\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": input_encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": input_encoding[\"attention_mask\"].flatten(),\n            \"labels\": labels.flatten(),\n        }\n\n    def print_samples(self, num_samples=5):\n        \"\"\"Prints a specified number of samples from the dataset.\"\"\"\n        print(f\"{'Index':<10}{'Wrong Sentence':<50}{'Correct Sentence'}\")\n        print(\"=\"*100)\n        for idx in range(min(num_samples, len(self.dataframe))):\n            wrong = self.dataframe.iloc[idx][\"wrong\"]\n            correct = self.dataframe.iloc[idx][\"correct\"]\n            print(f\"{idx:<10}{wrong:<50}{correct}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:18.056290Z","iopub.execute_input":"2024-05-30T04:27:18.056618Z","iopub.status.idle":"2024-05-30T04:27:18.074473Z","shell.execute_reply.started":"2024-05-30T04:27:18.056588Z","shell.execute_reply":"2024-05-30T04:27:18.073627Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# Adding special tokens to the tokenizer\nspecial_tokens_dict = {'additional_special_tokens': ['<extra_token_1>', '<extra_token_2>']}\ntokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Initialize the word-level dataset\nword_dataset = WordCorrectionDataset(words_df, tokenizer)\nword_dataset.print_samples(num_samples=5)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:18.075548Z","iopub.execute_input":"2024-05-30T04:27:18.075886Z","iopub.status.idle":"2024-05-30T04:27:18.097604Z","shell.execute_reply.started":"2024-05-30T04:27:18.075862Z","shell.execute_reply":"2024-05-30T04:27:18.096656Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Index     Wrong Word          Correct Word\n==================================================\n0         corrected           #misspelt\n1         آگاهي               آاهي\n2         آیات                آبات\n3         آب باشد             آبباشد\n4         آید                 آبد\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence_dataset = SentenceCorrectionDataset(sentences_df, tokenizer)\nsentence_dataset.print_samples(num_samples=5)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:28:53.116867Z","iopub.execute_input":"2024-05-30T04:28:53.117319Z","iopub.status.idle":"2024-05-30T04:28:53.123181Z","shell.execute_reply.started":"2024-05-30T04:28:53.117285Z","shell.execute_reply":"2024-05-30T04:28:53.122273Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Index     Wrong Sentence                                    Correct Sentence\n====================================================================================================\n0         اما گروه دوم افزایش تدریجی قابل پیش‌بینی و مشروط را دارای اثر‌گزاری بیشتری می‌داننداما گروه دوم افزایش تدریجی قابل پیش‌بینی و مشروط را دارای اثرگذاری بیشتری می‌دانند\n1         دانشگاه‌ها حوزه اثر‌گزاری چه قبل از انقلاب و چه بعد از انقلاب بوده‌انددانشگاه‌ها حوزه اثرگذاری چه قبل از انقلاب و چه بعد از انقلاب بوده‌اند\n2         به‌اندازه ظرفیت خودش بر بازار اثر‌گزار باشد       به‌اندازه ظرفیت خودش بر بازار اثرگذار باشد\n3         که تشکیل دادسرای امور بین‌الملل برای رسیدگی به این مهم اثر‌گزار باشدکه تشکیل دادسرای امور بین‌الملل برای رسیدگی به این مهم اثرگذار باشد\n4         در صورت نیاز نسبت به اصلاح نسبت به اثر‌گزاری متن اقدام تا پس از اصلاح برای امضا تقدیم حضور گردددر صورت نیاز نسبت به اصلاح نسبت به اثرگذاری متن اقدام تا پس از اصلاح برای امضا تقدیم حضور گردد\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=200,\n    per_device_train_batch_size=512,\n    per_device_eval_batch_size=512,\n    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n    save_steps=10_000,\n    save_total_limit=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    logging_steps=500,\n    report_to=\"none\",\n    fp16=True,  # Enable mixed precision training\n    dataloader_num_workers=4,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:18.098720Z","iopub.execute_input":"2024-05-30T04:27:18.099029Z","iopub.status.idle":"2024-05-30T04:27:18.128918Z","shell.execute_reply.started":"2024-05-30T04:27:18.099005Z","shell.execute_reply":"2024-05-30T04:27:18.127936Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=word_dataset,\n    eval_dataset=word_dataset,  # Ideally, use a separate validation set\n    tokenizer=tokenizer,\n)\n\n# Prepare dataloaders with Accelerator\ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\naccelerator = Accelerator(dataloader_config=dataloader_config)\n\ntrain_loader = DataLoader(word_dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True)\neval_loader = DataLoader(word_dataset, batch_size=training_args.per_device_eval_batch_size)\n\ntrain_loader, eval_loader, model = accelerator.prepare(\n    train_loader, eval_loader, model\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:27:18.130139Z","iopub.execute_input":"2024-05-30T04:27:18.130447Z","iopub.status.idle":"2024-05-30T04:27:18.424163Z","shell.execute_reply.started":"2024-05-30T04:27:18.130421Z","shell.execute_reply":"2024-05-30T04:27:18.423252Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Disable parallelism in tokenizers to avoid deadlocks\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# Train the model on sentence-level dataset\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:19:09.143253Z","iopub.execute_input":"2024-05-30T04:19:09.144071Z","iopub.status.idle":"2024-05-30T04:22:50.784652Z","shell.execute_reply.started":"2024-05-30T04:19:09.144037Z","shell.execute_reply":"2024-05-30T04:22:50.783718Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 03:39, Epoch 200/200]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.0, metrics={'train_runtime': 220.7309, 'train_samples_per_second': 3.624, 'train_steps_per_second': 0.906, 'total_flos': 547762470912000.0, 'train_loss': 0.0, 'epoch': 200.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,  # Reduced batch size\n    per_device_eval_batch_size=2,   # Reduced batch size\n    gradient_accumulation_steps=4,  # Increase gradient accumulation steps\n    save_steps=10_000,\n    save_total_limit=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    logging_steps=500,\n    report_to=\"none\",\n    fp16=True,  # Enable mixed precision training\n    dataloader_num_workers=4,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:30:32.293124Z","iopub.execute_input":"2024-05-30T04:30:32.293997Z","iopub.status.idle":"2024-05-30T04:30:32.320245Z","shell.execute_reply.started":"2024-05-30T04:30:32.293959Z","shell.execute_reply":"2024-05-30T04:30:32.319495Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=sentence_dataset,\n    eval_dataset=sentence_dataset,  # Ideally, use a separate validation set\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:30:32.934941Z","iopub.execute_input":"2024-05-30T04:30:32.935417Z","iopub.status.idle":"2024-05-30T04:30:32.957698Z","shell.execute_reply.started":"2024-05-30T04:30:32.935383Z","shell.execute_reply":"2024-05-30T04:30:32.956816Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare dataloaders with Accelerator\naccelerator = Accelerator()\ntrain_loader = DataLoader(sentence_dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True)\neval_loader = DataLoader(sentence_dataset, batch_size=training_args.per_device_eval_batch_size)\n\nmodel, train_loader, eval_loader = accelerator.prepare(\n    model, train_loader, eval_loader\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:30:33.153116Z","iopub.execute_input":"2024-05-30T04:30:33.153818Z","iopub.status.idle":"2024-05-30T04:30:33.179711Z","shell.execute_reply.started":"2024-05-30T04:30:33.153784Z","shell.execute_reply":"2024-05-30T04:30:33.178791Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Train the model on sentence-level dataset\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T04:30:33.349530Z","iopub.execute_input":"2024-05-30T04:30:33.350169Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='501' max='2205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 501/2205 12:48 < 43:45, 0.65 it/s, Epoch 0.23/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='3833' max='8820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3833/8820 08:25 < 10:57, 7.59 it/s]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"# Save the model\nmodel_save_path = \"./fine_tuned_t5_model\"\nmodel.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GPT2-PERSIAN","metadata":{}},{"cell_type":"code","source":"# Load GPT-2 model and tokenizer\nmodel_name = \"HooshvareLab/gpt2-fa\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,  # Reduced batch size\n    per_device_eval_batch_size=2,   # Reduced batch size\n    gradient_accumulation_steps=4,  # Increase gradient accumulation steps\n    save_steps=10_000,\n    save_total_limit=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    logging_steps=500,\n    report_to=\"none\",\n    fp16=True,  # Enable mixed precision training\n    dataloader_num_workers=4,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=sentence_dataset,\n    eval_dataset=sentence_dataset,  # Ideally, use a separate validation set\n    tokenizer=tokenizer,\n)\n# Prepare dataloaders with Accelerator\naccelerator = Accelerator()\ntrain_loader = DataLoader(sentence_dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True)\neval_loader = DataLoader(sentence_dataset, batch_size=training_args.per_device_eval_batch_size)\n\nmodel, train_loader, eval_loader = accelerator.prepare(\n    model, train_loader, eval_loader\n)\n\n# Print a sample output before training\ndef print_sample_output(model, tokenizer, sample_text):\n    model.eval()\n    inputs = tokenizer(sample_text, return_tensors=\"pt\").to(accelerator.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=512, num_beams=5, early_stopping=True)\n    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Sample input: {sample_text}\")\n    print(f\"Model output: {corrected_text}\")\n\nsample_text = sentences_df.iloc[0][\"wrong\"]\nprint_sample_output(model, tokenizer, sample_text)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Model and i-o T5","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, AutoTokenizer\n\nclass CombinedModel:\n    def __init__(self, model_path, words_path, words_df_path):\n        \"\"\"\n        Initializes the CombinedModel with the fine-tuned T5 model and the DistanceModel.\n        \n        Args:\n        model_path (str): Path to the directory containing the fine-tuned T5 model.\n        words_path (str): Path to the file containing Persian words.\n        words_df_path (str): Path to the file containing the correction DataFrame.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n        self.distance_model = DistanceModel(words_path, words_df_path)\n\n    def inference(self, sentence):\n        \"\"\"\n        Processes the input sentence using both the T5 model and the DistanceModel.\n        \n        Args:\n        sentence (str): The input sentence to be processed.\n        \n        Returns:\n        str: The processed sentence.\n        \"\"\"\n        # Step 1: Use the DistanceModel to clean the sentence\n        cleaned_sentence = self.distance_model.inference(sentence)\n        \n        # Step 2: Use the T5 model to generate the final output\n        input_ids = self.tokenizer.encode(cleaned_sentence, return_tensors='pt')\n        with torch.no_grad():\n            outputs = self.model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n        final_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        input_words = sentence.split()\n        if len(final_output_words) != len(input_words):\n            final_output_words = final_output_words[:len(input_words)]  # Truncate if longer\n            if len(final_output_words) < len(input_words):\n                final_output_words += input_words[len(final_output_words):]  # Pad if shorter\n        \n        return ' '.join(final_output_words)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nmodel_path = \"./fine_tuned_t5_model\"\nwords_path = \"path_to_words_file.txt\"\nwords_df_path = \"path_to_words_df.csv\"\n\ncombined_model = CombinedModel(model_path, words_path, words_df_path)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Model and i-o GPT","metadata":{}},{"cell_type":"code","source":"\nclass CombinedModel:\n    def __init__(self, model_path, words_path, words_df_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n        self.distance_model = DistanceModel(words_path, words_df_path)\n\n    def inference(self, sentence):\n        cleaned_sentence = self.distance_model.inference(sentence)\n        input_ids = self.tokenizer.encode(cleaned_sentence, return_tensors='pt')\n        with torch.no_grad():\n            outputs = self.model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n        final_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        final_output_words = final_output.split()\n        input_words = sentence.split()\n        if len(final_output_words) != len(input_words):\n            final_output_words = final_output_words[:len(input_words)]\n            if len(final_output_words) < len(input_words):\n                final_output_words += input_words[len(final_output_words):]\n        \n        return ' '.join(final_output_words)\n\n# Example usage\nmodel_path = \"./fine_tuned_gpt2_model\"\nwords_path = \"path_to_words_file.txt\"\nwords_df_path = \"path_to_words_df.csv\"\n\ncombined_model = CombinedModel(model_path, words_path, words_df_path)","metadata":{},"execution_count":null,"outputs":[]}]}